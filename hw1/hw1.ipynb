{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 1: Web log data wrangling\n",
    "\n",
    "### This assignment has been originally developed for [UC Berkeley CS 186 course](http://www.cs186berkeley.net/); we use it for COMS4037 with their gracious permission\n",
    "\n",
    "Please, refer to the HW1 [README](https://github.com/WITS-COMS4037/hw/tree/master/hw1) for the full details for this assignment.\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Jupyter Notebook with IPython\n",
    "\n",
    "Jupyter Notebook is a web-based interactive computing system that allows you to mix code and text in [markdown format](https://en.wikipedia.org/wiki/Markdown) in a single document. A notebook consists of a sequence of cells that can be run by hitting Shift-Enter on the keyboard.\n",
    "\n",
    "In HW1, you will primarily use code cells to work with IPython code. You can find a tour and pointers to documentation in the `Help` menu at the top of this page.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "\n",
    "The data are web logs that were produced by an Apache web server; each line represents a request to the server that hosted a video from 2002."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/calvin/coms4037/hw1/'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we assume that the dataset is in ~/coms4037/hw1 \n",
    "import os\n",
    "DATA_DIR = os.environ['HOME'] + '/coms4037/hw1/'\n",
    "#DATA_DIR = os.environ['HOME'] + '/coms4037/hw1/datasets/'\n",
    "#DATA_DIR = os.environ['HOME'] + '/COMS7043/hw/hw1/Datasets/'\n",
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.172.72.131 - - [02/Jan/2003:02:06:41 -0700] \"GET /random/html/riaa_hacked/ HTTP/1.0\" 200 10564 \"-\" \"Mozilla/4.0 (compatible; MSIE 5.5; Windows NT 4.0; WWP 17 August 2001)\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look at the first line of a file to see what the data looks like\n",
    "with open(DATA_DIR + \"web_log_small.log\") as log_file:\n",
    "    sample_line = log_file.readline()\n",
    "\n",
    "print( sample_line )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in the format called \"Combined Log Format\"; you can find a description of each of the fields [here](https://httpd.apache.org/docs/1.3/logs.html#common)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to view the first line of the dataset is to run a shell command using the IPython's [`! operator`](https://ipython.org/ipython-doc/3/interactive/reference.html#system-shell-access), as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.172.72.131 - - [02/Jan/2003:02:06:41 -0700] \"GET /random/html/riaa_hacked/ HTTP/1.0\" 200 10564 \"-\" \"Mozilla/4.0 (compatible; MSIE 5.5; Windows NT 4.0; WWP 17 August 2001)\"\r\n"
     ]
    }
   ],
   "source": [
    "!head -1 {DATA_DIR}web_log_small.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----------\n",
    "\n",
    "## Your Assignment\n",
    "\n",
    "You need to complete the implementation of the `process_logs` function in the code given below to meet the specification described in the README file. In doing so, you can add any helper functions. You may also use any of Python 2's standard libraries; you should not, however, use any external libraries.\n",
    "\n",
    "You need to ensure that your code will scale to datasets that are bigger than main memory, regardless of how large or skewed the dataset is or how much memory the executing machine has.  Thus, you should avoid keeping data structures of unbounded size, such as\n",
    "\n",
    "- a list of every line in the dataset\n",
    "- a dictionary with an key for every IP address\n",
    "\n",
    "in memory. \n",
    "\n",
    "### Important\n",
    "\n",
    "To ensure proper grading, make sure that all of your log processing code (including `import` statements) is between the **BEGIN/END STUDENT CODE** cells. Do not modify or remove either of these cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * BEGIN STUDENT CODE *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import apachetime\n",
    "import time\n",
    "# new imports\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def apache_ts_to_unixtime( ts ):\n",
    "    \"\"\"\n",
    "    @param ts - a Apache timestamp string, e.g. '[02/Jan/2003:02:06:41 -0700]'\n",
    "    @returns int - a Unix timestamp in seconds\n",
    "    \"\"\"\n",
    "    dt = apachetime.apachetime( ts )\n",
    "    unixtime = time.mktime( dt.timetuple() )    \n",
    "    \n",
    "    return int( unixtime )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_logs (dataset_iter ):\n",
    "    \"\"\"\n",
    "    Processes the input stream, and outputs the CSV files described in the README.    \n",
    "    This is the main entry point for your assignment.\n",
    "    \n",
    "    @param dataset_iter - an iterator of Apache log lines.\n",
    "    \"\"\"\n",
    "    # FIX ME\n",
    "    with open( \"hits.csv\", \"w+\" ) as hits_file:\n",
    "        header = \"ip,timestamp\\n\" #new line for writing header line to the csv file\n",
    "        hits_file.write(header) #new line\n",
    "        for i, line in enumerate( dataset_iter ):\n",
    "            #regex = '([(\\d\\.)]+) (.*) (.*) (\\[.*\\]) \"(.*)\" (.*) (.*) \"(.*)\" \"(.*)\"'\n",
    "            #logs = re.match(regex, line).groups()\n",
    "            #timestamp = apache_ts_to_unixtime((logs[3]))\n",
    "            \n",
    "            logs = line.split() #splits each lines content where there is a space\n",
    "            timestamp = logs[3] + logs[4] #combination to form timestamp with time zone included\n",
    "            unix_time = apache_ts_to_unixtime(timestamp) #convert timestamp to unix time\n",
    "            \n",
    "            hits_file.write(str(logs[0])+ \",\" + str(unix_time) + \"\\n\")            \n",
    "            if i % 1e5 == 0:\n",
    "                print( i ),\n",
    "        \n",
    "        print( \"Done\" )\n",
    "        print (\"hits.csv done\")\n",
    "    \n",
    "    #creation of sessions.csv file\n",
    "    \n",
    "    # Sort the data based and create a new csv file with the sorted content from hits file\n",
    "    !tail -n +2 hits.csv | sort -o hits_sorted.csv # tail command reads all but the header line\n",
    "    with open(\"sessions.csv\", \"w+\") as sessions_file: \n",
    "        sessions_file.write(\"ip,session_length,num_hits\\n\")\n",
    "        with open(\"hits_sorted.csv\", \"r\") as hits_sorted_file: #reading from the sorted hits file\n",
    "            #Initialize parameters\n",
    "            started = 0 #time started\n",
    "            ended = 0 #ending time for the session\n",
    "            num_hits = 0 # number of hits\n",
    "            prev_ip = None #initialized to none as no row has been read yet\n",
    "\n",
    "            for row in hits_sorted_file:\n",
    "                row_data = row.split(\",\") #splits each line/row based on presence of a comma\n",
    "                ip = row_data[0] #picks out or reads IP address\n",
    "                t_stamp = int(row_data[1]) #picks out unix time value\n",
    "\n",
    "                if ip != prev_ip:\n",
    "                    if num_hits >= 1:\n",
    "                        sessions_file.write(prev_ip +\",\"+ str(ended - started) +\",\"+ str(num_hits)+ \"\\n\")\n",
    "                    #re-evaluate initialized variables\n",
    "                    prev_ip = ip\n",
    "                    started = t_stamp\n",
    "                    ended = t_stamp\n",
    "                    num_hits = 1\n",
    "                else:\n",
    "                    if t_stamp - ended <= 1800: #1800 seconds represents 30 minutes for session determination\n",
    "                        num_hits +=1\n",
    "                        ended = t_stamp\n",
    "                    else: #If difference in time > 1800 seconds, then a new session is started\n",
    "                        sessions_file.write(prev_ip +\",\"+ str(ended - started) +\",\"+ str(num_hits)+ \"\\n\")\n",
    "                        started = t_stamp\n",
    "                        ended = t_stamp\n",
    "                        num_hits = 1\n",
    "            sessions_file.write(prev_ip +\",\"+ str(ended - started) +\",\"+ str(num_hits)+ \"\\n\")\n",
    "\n",
    "    print \"sessions.csv done\"\n",
    "    \n",
    "    \n",
    "    # creation of session_length_plot.csv\n",
    "    !tail -n +2 sessions.csv | sort -t \",\" -k2,2n > sessions_sorted.csv #sorting using session_length column\n",
    "\n",
    "    with open(\"session_length_plot.csv\", \"w+\") as histogram_file:\n",
    "        histogram_file.write(\"left,right,count\\n\")\n",
    "        with open(\"sessions_sorted.csv\", \"rb\") as sessions_data:\n",
    "            #Initialization of params\n",
    "            left = 0\n",
    "            right = 2\n",
    "            num = 0\n",
    "\n",
    "            for session_row in sessions_data:\n",
    "                sess_row_data = session_row.split(\",\")\n",
    "                session_length = int(sess_row_data[1])              \n",
    "                if session_length < right:\n",
    "                    num += 1\n",
    "                else:\n",
    "                    if num >= 1:\n",
    "                        histogram_file.write(str(left) +\",\"+ str(right) +\",\"+ str(num) +\"\\n\")\n",
    "                    left = right\n",
    "                    right = right * 2\n",
    "                    num = 1\n",
    "            histogram_file.write(str(left) +\",\"+ str(right) +\",\"+ str(num) +\"\\n\")\n",
    "\n",
    "    \n",
    "    print \"session_length_plot.csv done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * END STUDENT CODE *\n",
    "\n",
    "------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_logs_small( ):\n",
    "    \"\"\"\n",
    "    Runs the process_logs function with the small dataset (186 MB).\n",
    "    \"\"\"        \n",
    "    with open( DATA_DIR + \"web_log_small.log\" ) as log_file:\n",
    "        process_logs( log_file )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100000 200000 300000 400000 500000 600000 700000 800000 900000 Done\n",
      "hits.csv done\n",
      "sessions.csv done\n",
      "session_length_plot.csv done\n",
      "CPU times: user 15.4 s, sys: 864 ms, total: 16.2 s\n",
      "Wall time: 20.8 s\n"
     ]
    }
   ],
   "source": [
    "%time process_logs_small( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def process_logs_large( ):\n",
    "    \"\"\"\n",
    "    Runs the process_logs function on the full dataset.  The code below \n",
    "    performs a streaming unzip of the compressed dataset which is (158MB). \n",
    "    This saves the 1.6GB of disk space needed to unzip this file onto disk.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile( DATA_DIR + \"web_log_large.zip\" ) as z:\n",
    "        fname = z.filelist[0].filename\n",
    "        f = z.open( fname )\n",
    "        process_logs( f )\n",
    "        f.close( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100000 200000 300000 400000 500000 600000 700000 800000 900000 1000000 1100000 1200000 1300000 1400000 1500000 1600000 1700000 1800000 1900000 2000000 2100000 2200000 2300000 2400000 2500000 2600000 2700000 2800000 2900000 3000000 3100000 3200000 3300000 3400000 3500000 3600000 3700000 3800000 3900000 4000000 4100000 4200000 4300000 4400000 4500000 4600000 4700000 4800000 4900000 5000000 5100000 5200000 5300000 5400000 5500000 5600000 5700000 5800000 5900000 6000000 6100000 6200000 6300000 6400000 6500000 6600000 6700000 6800000 6900000 7000000 7100000 7200000 7300000 7400000 7500000 7600000 7700000 7800000 7900000 8000000 8100000 8200000 Done\n",
      "hits.csv done\n",
      "sessions.csv done\n",
      "session_length_plot.csv done\n",
      "CPU times: user 2min 37s, sys: 6.19 s, total: 2min 43s\n",
      "Wall time: 3min 26s\n"
     ]
    }
   ],
   "source": [
    "%time process_logs_large( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---------------\n",
    "\n",
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide reference output only for the small dataset. The function `diff_against_reference()` produces `.diff` files if there's a difference between your output and the referrence output.\n",
    "\n",
    "If you're unfamiliar with the format of `diff`'s output, you can read about it [here](https://en.wikipedia.org/wiki/Diff_utility#Usage).\n",
    "\n",
    "There are other diff utilities that produce more convenient output, making it easier to see the differences between the input files. If you're interested, you might try:\n",
    "\n",
    "```\n",
    "$ vimdiff hits.csv ~/coms4037/hw1/ref_output_small/hits.csv\n",
    "OR\n",
    "$ git diff hits.csv ~/coms4037/hw1/ref_output_small/hits.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#ref_output_dir = DATA_DIR + \"ref_output_small/\"\n",
    "ref_output_dir = DATA_DIR + \"reference_output_web_log_small/\"\n",
    "\n",
    "def _diff_helper( f, unordered=False ):\n",
    "    \"\"\"\n",
    "    @param f (str) - filename to diff with reference output\n",
    "    @param unordered (bool) - whether the ordering of the lines matters\n",
    "    \"\"\"\n",
    "    if not os.path.isfile( f ):\n",
    "        print \"FAIL - {} does not exist.\".format( f )\n",
    "        return\n",
    "    \n",
    "    if unordered:\n",
    "        tmp1 = !mktemp\n",
    "        tmp1 = tmp1[0]\n",
    "        !sort {f} > {tmp1}\n",
    "        !sort {ref_output_dir + f} | diff {tmp1} - > {f}.diff\n",
    "    else:\n",
    "        !diff {f} {ref_output_dir + f} > {f}.diff\n",
    "    \n",
    "    success = _exit_code == 0\n",
    "    if success:\n",
    "        !rm {f}.diff\n",
    "        print \"PASS - {} matched reference output.\".format( f )\n",
    "    else:\n",
    "        print \"FAIL - {} did not match reference output. See {}.diff.\".format( f, f )\n",
    "        \n",
    " \n",
    "def diff_against_reference( ):\n",
    "    \"\"\"\n",
    "    Compares the output files in the current directory with the reference output.\n",
    "    If there is a difference, writes a \".diff\" file, e.g. hits.csv.diff.\n",
    "    \"\"\" \n",
    "    _diff_helper( \"hits.csv\" )\n",
    "    _diff_helper( \"sessions.csv\", unordered=True )\n",
    "    _diff_helper( \"session_length_plot.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100000 200000 300000 400000 500000 600000 700000 800000 900000 Done\n",
      "hits.csv done\n",
      "sessions.csv done\n",
      "session_length_plot.csv done\n",
      "PASS - hits.csv matched reference output.\n",
      "PASS - sessions.csv matched reference output.\n",
      "PASS - session_length_plot.csv matched reference output.\n"
     ]
    }
   ],
   "source": [
    "process_logs_small( )\n",
    "diff_against_reference( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Testing Memory Usage\n",
    "\n",
    "For additional testing, we've included a script that\n",
    " - (1) makes sure all of your log processing code is between the BEGIN/END STUDENT CODE CELLS above, so it will work with the test code\n",
    " - (2) runs your code with a memory cap of 1MB. If you see a `MemoryError`, it means that your code is not doing appropriate streaming and/or divide-and-conquer!\n",
    " \n",
    "Make sure to save your notebook (`File > Save and Checkpoint` or Ctrl-S) before running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook hw1.ipynb to python\n",
      "Running process_logs_large()\n",
      "\u001b]0;IPython: hw/hw1\u00070 100000 200000 300000 400000 500000 600000 700000 800000 900000 1000000 1100000 1200000 1300000 1400000 1500000 1600000 1700000 1800000 1900000 2000000 2100000 2200000 2300000 2400000 2500000 2600000 2700000 2800000 2900000 3000000 3100000 3200000 3300000 3400000 3500000 3600000 3700000 3800000 3900000 4000000 4100000 4200000 4300000 4400000 4500000 4600000 4700000 4800000 4900000 5000000 5100000 5200000 5300000 5400000 5500000 5600000 5700000 5800000 5900000 6000000 6100000 6200000 6300000 6400000 6500000 6600000 6700000 6800000 6900000 7000000 7100000 7200000 7300000 7400000 7500000 7600000 7700000 7800000 7900000 8000000 8100000 8200000 Done\n",
      "hits.csv done\n",
      "sessions.csv done\n",
      "session_length_plot.csv done\n",
      "Memory Test Done.\n"
     ]
    }
   ],
   "source": [
    "!bash test_memory_usage.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
